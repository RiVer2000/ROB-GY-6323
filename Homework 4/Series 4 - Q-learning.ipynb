{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-learning with neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a few packages we need to import\n",
    "%matplotlib widget\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import matplotlib.animation as animation\n",
    "import IPython \n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "\n",
    "import pendulum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this homework is to implement the Q-learning with a neural network for the Q function to solve the inverted pendulum problem.\n",
    "\n",
    "<img src='pendulum.png' width=\"120\">\n",
    "\n",
    "In the following, we write $x = \\begin{pmatrix} \\theta \\\\ \\dot{\\theta} \\end{pmatrix}$ as the vector of states of the system.\n",
    "\n",
    "## System dynamics\n",
    "* The system dynamics is implemented in the `pendulum.py` function. The dynamics is implemented in `pendulum.step`.\n",
    "* The allowed control inputs are $[-5,0,5]$\n",
    "\n",
    "## Cost function\n",
    "The goal is to find a policy that minimizes the following cost\n",
    "$$\\min \\sum_{n=0}^N \\alpha^n g(x,u)$$\n",
    "where\n",
    "$$g(x,v,u) = 0.01*(1-\\cos(x-\\pi))^2 + 0.001* v^2 + 0.00001*u^2$$\n",
    "which gives a high cost for states far from $\\pi$ (i.e. far from the inverted position) or states with non zero velocity or high controls\n",
    "\n",
    "\n",
    "\n",
    "## Q-learning algorithm to implement\n",
    "For each episode:\n",
    "* Initialize the episode $x_0 = [0,0]$\n",
    "* For each step of the episode:\n",
    "    * Select $u_n$ using an $\\epsilon$-greedy policy\n",
    "    * Compute the next state $x_{n+1}$\n",
    "    * Compute the target $y_n = g(x_n,u_n) + \\alpha \\min_a Q(x_{n+1},a)$\n",
    "    * Do one SGD step on the neural network parameters to minimize $(Q(x,u) - y_t)^2$\n",
    "\n",
    "\n",
    "## Parameters:\n",
    "* Episode length 100 steps\n",
    "* Discount factor $\\alpha = 0,99$\n",
    "* Learning rate (for SGD) 0.1\n",
    "* $\\epsilon = 0.1$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using PyTorch\n",
    "You need to install and use PyTorch for the neural network and do the optimization. \n",
    "\n",
    "You may want to use the following functions:\n",
    "* [`torch.optim.SGD`](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html)\n",
    "* [`torch.nn.MSELoss`](https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html)\n",
    "\n",
    "The neural network is given below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## we define the neural network to be used for Q-learning\n",
    "## 2 hidden layers with 64 nodes\n",
    "## 2 inputs (state)\n",
    "## 3 outputs for the 3 possible controls\n",
    "D_in, H, D_out = 2, 64, 3\n",
    "\n",
    "q_function = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in, H),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(H, H),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(H, H),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(H, D_out)\n",
    ")\n",
    "\n",
    "## we initialize the network parameters to 0\n",
    "for params in q_function.parameters():\n",
    "    params = torch.zeros_like(params)\n",
    "\n",
    "\n",
    "### possible controls\n",
    "possible_controls = np.array([-5.,0.,5.])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions:\n",
    "1. Implement the Q-learning algorithm described above\n",
    "2. Test that it works with and without pushes using the code below\n",
    "3. Plot the cost per episode (to visualize learning)\n",
    "4. Plot the learned value function (in 2D as a function of pendulum position and velocity) as well as the policy.\n",
    "5. Describe the algorithm and put the plots in a short report (max 2 pages) and include a video of the pendulum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "You can test your results with the code below which use the Q-function to create a controller send to the `animate_robot` function.\n",
    "You can choose to save also a movie of the animation and toggle the animation with a disturbance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x0 = np.zeros((2,1))\n",
    "# def controller(x):\n",
    "#     u_pred = torch.argmin(q_function(torch.as_tensor(x, dtype=torch.float).unsqueeze(0))).item()\n",
    "#     u = possible_controls[u_pred]\n",
    "#     return u\n",
    "    \n",
    "# pendulum.animate_robot(x0,controller,push=True, save_movie=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(x, u):\n",
    "    return (0.01 * (1 - np.cos(x[0] - np.pi)) + 0.001 * x[1]**2 + 0.00001 * u**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make device agnostic code\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "q_function.to(device=device)\n",
    "# np.random.seed(42)\n",
    "# torch.manual_seed(42)\n",
    "N = 100\n",
    "MAX_ITER = 5000\n",
    "alpha = 0.99\n",
    "lr = 0.1\n",
    "epsilon = 0.1\n",
    "\n",
    "# loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "optimizer = torch.optim.SGD(q_function.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training in Progress:   0%|          | 0/5000 [00:00<?, ?it/s]/tmp/ipykernel_15915/3652728005.py:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  xi = torch.tensor(xip1, device=device, dtype=torch.float)\n",
      "Training in Progress: 100%|██████████| 5000/5000 [17:56<00:00,  4.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "values of x0: tensor([5.5513, 0.6516], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "for _ in tqdm(range(MAX_ITER), desc=f'Training in Progress'):\n",
    "    xi = torch.tensor(np.zeros((2, )), device=device, dtype=torch.float)\n",
    "    for _ in range(N):\n",
    "        forward_pass = q_function.forward(xi.unsqueeze(0))\n",
    "        if torch.rand(1).item() > epsilon:\n",
    "            ui = torch.argmin(forward_pass).item()\n",
    "        else:\n",
    "            ui = torch.randint(0, 3, (1,)).item()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            xip1 = torch.tensor(pendulum.step(\n",
    "                x=xi.cpu().numpy(), u=possible_controls[ui]), device=device, dtype=torch.float)\n",
    "\n",
    "            yi = torch.tensor((cost(x=xi.cpu().numpy(), u=possible_controls[ui]) + (alpha * torch.min(\n",
    "                q_function.forward(xip1.unsqueeze(0))).item())), device=device, dtype=torch.float)\n",
    "\n",
    "        loss = loss_fn(forward_pass.squeeze()[ui], yi)\n",
    "        if ~torch.isnan(loss):\n",
    "            pass\n",
    "        # use the optimizer object to zero all of the gradients for the variables it will update,\n",
    "        # i.e. the weights of the model. Checkout docs of torch.autograd.backward for more details.\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # compute gradient of the loss with respect to model parameters (backward autodiff)\n",
    "        loss.backward()\n",
    "\n",
    "        # call the step function of the optimizer to make one update of the parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        xi = torch.tensor(xip1, device=device, dtype=torch.float)\n",
    "\n",
    "print(f\"values of x0: {xi}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'q_function' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m x \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m))  \u001b[38;5;66;03m# Replace n_rows and n_columns with actual dimensions\u001b[39;00m\n\u001b[1;32m     17\u001b[0m x[:,\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m x0[:,\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m---> 19\u001b[0m \u001b[43mpendulum\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43manimate_robot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontroller\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpush\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_movie\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Classwork/ROB-GY-6323/Homework 4/pendulum.py:33\u001b[0m, in \u001b[0;36manimate_robot\u001b[0;34m(x0, controller, push, save_movie)\u001b[0m\n\u001b[1;32m     31\u001b[0m x[:,\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m x0[:,\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(N\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m---> 33\u001b[0m     u \u001b[38;5;241m=\u001b[39m \u001b[43mcontroller\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m     x[:,i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m step(x[:,i],u)\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m push \u001b[38;5;129;01mand\u001b[39;00m (i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m45\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m70\u001b[39m):\n",
      "Cell \u001b[0;32mIn[10], line 9\u001b[0m, in \u001b[0;36mcontroller\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcontroller\u001b[39m(x):\n\u001b[0;32m----> 9\u001b[0m     u_pred \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmin(\u001b[43mq_function\u001b[49m(torch\u001b[38;5;241m.\u001b[39mas_tensor(x, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat, device\u001b[38;5;241m=\u001b[39mdevice)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)))\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     10\u001b[0m     u \u001b[38;5;241m=\u001b[39m possible_controls[u_pred]\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m u\n",
      "\u001b[0;31mNameError\u001b[0m: name 'q_function' is not defined"
     ]
    }
   ],
   "source": [
    "# print(xi.shape)\n",
    "# print(xi)\n",
    "\n",
    "# x0 = np.zeros((2, 1))\n",
    "x0 = np.array([5.5513, 0.6516]).reshape(2, 1)\n",
    "\n",
    "\n",
    "def controller(x):\n",
    "    u_pred = torch.argmin(q_function(torch.as_tensor(x, dtype=torch.float, device=device).unsqueeze(0))).item()\n",
    "    u = possible_controls[u_pred]\n",
    "    return u\n",
    "\n",
    "# assert(x0.shape == (2,))\n",
    "# assert(x0.shape == 2)\n",
    "assert(x0.shape[0]==2)\n",
    "x = np.zeros((2, 2))  # Replace n_rows and n_columns with actual dimensions\n",
    "x[:,0] = x0[:,0]\n",
    "\n",
    "pendulum.animate_robot(x0, controller, push=False, save_movie=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def traj_controller():\n",
    "    for _ in tqdm(range(5000), desc=f'Running Simulation'):\n",
    "        xi = torch.from_numpy(np.zeros((2, )), dtype=torch.float).to(device=device, dtype=torch.float)\n",
    "        for _ in range(100):\n",
    "            # First we do the forward pass\n",
    "            forward_pass = q_function.forward(xi.unsqueeze(0))\n",
    "            # implement epsilon greedy policy\n",
    "            if torch.rand(1).item() > epsilon:\n",
    "                ui = torch.argmin(forward_pass).item()\n",
    "            else:\n",
    "                ui = torch.randint(0, 3, (1,)).item()\n",
    "\n",
    "            # No gradient calculation needed\n",
    "            with torch.no_grad():\n",
    "                xip1 = torch.from_numpy(pendulum.step(x=xi.cpu().numpy(), u=possible_controls[ui])).to(device=device, dtype=torch.float)\n",
    "                cost_value = cost(x=xi.cpu().numpy(), u=possible_controls[ui])\n",
    "                q_min_value = torch.min(q_function.forward(xip1.unsqueeze(0))).item()\n",
    "                total_value = cost_value + (alpha * q_min_value)\n",
    "                yi = torch.tensor(total_value, device=device, dtype=torch.float)\n",
    "            \n",
    "            loss = loss_fn(forward_pass.squeeze()[ui], yi)\n",
    "            # Check if the loss is nan\n",
    "            if ~torch.isnan(loss):\n",
    "                pass\n",
    "\n",
    "            # Zero all the gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Compute the gradients\n",
    "            loss.backward()\n",
    "\n",
    "            # Update the weights\n",
    "            optimizer.step()\n",
    "\n",
    "            xi = torch.from_numpy(xip1.cpu(), dtype=torch.float).to(device=device, dtype=torch.float)\n",
    "\n",
    "        # print(f\"values of x0: {xi}\")\n",
    "    return xi\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
